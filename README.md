# Entendiendo _Machine Learning_
_Traducido y documentado por Alejandro V√°zquez B._

## **Introducci√≥n**

## **Cap√≠tulo 1:** ¬øQu√© es _Machine Learning?_


Para entender qu√© es el _Machine Learning_ primero hay que hablar del termino: **Inteligencia Artifical** (IA). Hoy en d√≠a cuando la gente habla de _IA's_, suele referirse (sin saberlo) al _Machine Learning_.

La Inteligencia Artificial es un conjunto de herramientas para hacer que las computadoras tengan un "comportamiento" inteligente. Estas herramientas abarcan subcampos tales como la rob√≥tica y el _Machine Learning_.

![IA y Machine Learning](https://i.imgur.com/KwPW0uA.png)

Definir _Machine Learning_ no es una tarea sencilla, tiene muchas aplicaciones y se superpone con otros campos. Esto combinado con su exponencial crecimiento ha hecho que sus l√≠mites sean borrosos.

Nos gusta definir al _Machine Learning_ como un conjunto de herramientas que permiten realizar inferencias y predicciones a partir de datos.

Comparemos las tareas de inferencia y predicci√≥n para comprender mejor lo que puede hacer el _Machine Learning_.


---
### 1.1 ¬øQu√© puede hacer el _Machine Learning_?
---

- **Predice** eventos futuros:
    - Llover√° ma√±ana?
        - S√≠ (75% de probabilidad) 

- **Infiere** las causas de los eventos y comportamientos:
    - Por qu√© llueve?
        - √âpoca del a√±o, humedad, temperatura, ubicaci√≥n, etc.

- **Infiere** patrones:
    - Cu√°les son los diferentes tipos de condiciones clim√°ticas?
        - Lluvias, d√≠as soleados, neblina, etc.


Estas tareas pueden trabajar juntas porque las inferencias ayudan a hacer predicciones, sin embargo, requieren diferentes tipos de aprendizaje autom√°tico.


---
### 1.2 ¬øC√≥mo funciona el _Machine Learning_?
---
Los m√©todos de aprendizaje autom√°tico se basan principalmente en las las estad√≠sticas y la inform√°tica. Es una herramienta extremadamente poderosa debido a que le brinda a las computadoras la capacidad de aprender sin ser programadas expl√≠citamente para hacerlo. Es decir, la computadora puede aprender sin instrucciones paso a paso.
Esencialmente el aprendizaje autom√°tico aprende patrones de datos existentes y los aplica a nuevos datos. Por ejemplo, puede procesar correos electr√≥nicos archivados para saber c√≥mo se ve el spam por s√≠ solo, despu√©s, usando lo que aprendi√≥, puede detectar spam en nuevos correos electr√≥nicos. Es necesario aclarar que para que el aprendizaje autom√°tico sea exitoso, necesita datos de alta calidad.

**En resumen:**
- Es producto de una mezcla interdisciplinaria de estad√≠sticas e inform√°tica
- Habilidad de aprender sin ser programada para ello
- Aprende patrones de datos existentes y los aplica a nuevos datos
- Requiere datos de alta calidad


---
### 1.3 Relaci√≥n con la ciencia de datos
---
La ciencia de datos se trata de descubrir y comunicar informaci√≥n a partir de los datos. El aprendizaje autom√°tico suele ser una herramienta importante para el trabajo de ciencia de datos, especialmente para hacer predicciones a partir de datos.

![Ciencia de datos y Machine Learning](https://i.imgur.com/Lp1Qp9a.png)

---
1.4 Modelos de _Machine Learning_
---
Un modelo de aprendizaje autom√°tico es una representaci√≥n estad√≠stica de un proceso del mundo real, como la forma en que reconocemos a los gatos o los cambios de hora en el tr√°fico.

Un proceso se modela utilizando datos. El modelo recibe datos de entrada que lo alimentan y √©ste crea una salida.

![Modelo ML](https://i.imgur.com/PlIIvok.png)

Por ejemplo, si hacemos un modelo basado en datos hist√≥ricos de tr√°fico, podemos ingresar una fecha futura en el modelo para predecir qu√© tan pesado ser√° el tr√°fico ma√±ana por la tarde. 

![Modelo ML](https://i.imgur.com/noFs5V9.png)


---
### 1.5 Tipos de aprendizaje
---
Anteriormente se dijo mencion√≥ que el _Machine Learning_ "aprende" patrones de datos existentes y los aplica a nuevos, dichos datos existentes son llamados "**datos de entrenamiento**". Cuando se est√° construyendo un modelo y aprendiendo de los datos de entrenamiento, lo llamamos "entrenar un modelo". Esto puede tomar desde nanosegundos, hasta semanas, dependiendo del tama√±o de los datos.

Los datos de entrenamiento marcan la principal diferencia entre los tipos existentes de m√©todos de aprendizaje.

---
#### 1.5.1 Aprendizaje Reforzado
---

  El aprendizaje reforzado es utilizado para decidir acciones secuenciales, como un robot que decide su camino o su pr√≥ximo movimiento en un juego de ajedr√©z. No es tan com√∫n como el resto de m√©todos de aprendizaje y utiliza matem√°ticas complejas, como la teor√≠a de juegos. El aprendizaje supervisado y el no supervisado son los m√°s comunes.

---
#### 1.5.2 Aprendizaje Supervisado
---
Nos gustar√≠a entrenar un modelo para predecir si un paciente tiene una enfermedad card√≠aca, por lo que se capturan registros existentes de pacientes que han experimentado dolores en el pecho y han sido examinados para detectar enfermedades del coraz√≥n. Nuestra variable objetivo (_target objective_) es "_Heart Disease_" (enfermedad card√≠aca), porque esto es lo que queremos predecir.

![Datos de entrenamiento para t√©cnica de aprendizaje supervisado](https://i.imgur.com/q8z54Ck.png)

Los valores "True" y "False" son etiquetas (_labels_) para la variable objetivo, indican si un paciente tiene o no una enfermedad card√≠aca. Las etiquetas no tienen que estar en ese formato, podr√≠an ser n√∫meros o categor√≠as.

![Datos de entrenamiento para t√©cnica de aprendizaje supervisado objetivo](https://i.imgur.com/1Edquyo.png)


![Datos de entrenamiento para t√©cnica de aprendizaje supervisado etiqueta](https://i.imgur.com/Okv9mQk.png)

Las filas son las observaciones o ejemplos de los que aprender√° nuestro modelo, deber√≠amos obtener tantas como sea posible.

![Datos de entrenamiento para t√©cnica de aprendizaje supervisado filas](https://i.imgur.com/Kkxa9Iw.png)


Y estas columnas, son "caracter√≠sticas" (_features_), las caracter√≠sticas son diferentes piezas de informaci√≥n que pueden ayudar a predecir el objetivo. La edad, el colesterol y los h√°bitos de fumar son factores conocidos de enfermedades del coraz√≥n.

![Datos de entrenamiento para t√©cnica de aprendizaje supervisado caracter√≠sticas](https://i.imgur.com/dPYXq7H.png)

La magia del aprendizaje autom√°tico es que podemos analizar muchas caracter√≠sticas  a la vez, incluso aquellas de las que no estamos seguros, y encontrar relaciones entre diferentes caracter√≠sticas.

Ingresamos etiquetas y caracter√≠sticas para entrenar el modelo y una vez que se realiza el entrenamiento, podemos darle al modelo una nueva entrada. En nuestro caso, un nuevo paciente.

![Datos de entrenamiento para t√©cnica de aprendizaje supervisado nueva entrada](https://i.imgur.com/KVi1PPg.png)

Las caracter√≠sticas alimentan el modelo y a su vez, el modelo genera su predicci√≥n.

![Datos de entrenamiento para t√©cnica de aprendizaje supervisado nueva entrada](https://i.imgur.com/xiR1MlI.png)

Es as√≠ como funciona esta t√©cnica de aprendizaje autom√°tico, utilizando datos existentes de entrenamiento "etiquetados" para alimentar el modelo y en funci√≥n a dichas etiquetas, se realiza una predicci√≥n de nuestro objetivo. Por ejemplo, sab√≠amos si los pacientes anteriores ten√≠an enfermedades del coraz√≥n en funci√≥n de las etiquetas "True" y "False".

---
#### 1.5.3 Aprendizaje No Supervisado
---

A diferencia del **Aprendizaje Supervisado**, el **Aprendizaje No Supervisado** no se vale de etiquetas, sino √∫nicamente de las caracter√≠sticas.
Por lo general, tareas como la detecci√≥n de anomal√≠as y la agrupaci√≥n (_clustering_), que divide los datos en grupos seg√∫n la similitud. Exploremos esto con nuestro conjunto de datos.

Existen diferentes tratamientos para las enfermedades del coraz√≥n, diferentes tipos de pacientes responden mejor o peor a determinados tratamientos. Podemos utilizar el **Aprendizaje No Supervisado** para comprender los diferentes tipos de pacientes que tenemos.

Filtremos nuestro conjunto de datos para incluir s√≥lo los pacientes con enfermedades card√≠acas. Podemos pasarlo a un modelo de agrupamiento y obtener categor√≠as de pacientes basadas en la similitud de caracter√≠sticas.

![Datos de entrenamiento para t√©cnica de aprendizaje supervisado agrupamiento](https://i.imgur.com/tMGrgEH.png)

Por ejemplo, una categor√≠a podr√≠a ser pacientes con colesterol alto y nivel de az√∫car en la sangre de un cierto rango de edad. Tenga en cuenta que no conoc√≠amos estas categor√≠as e, incluso, la cantidad de categor√≠as antes de ejecutar esto. Con esta salida podemos agrupar pacientes e investigar mejores tratamientos para cada grupo.

Ahora, con un nuevo paciente, podemos ingresar las caracter√≠sticas en el modelo y obtener el tipo de paciente en el que encajan mejor.

![Datos de entrenamiento para t√©cnica de aprendizaje supervisado despu√©s del entrenamiento no supervisado](https://i.imgur.com/3f6baPV.png)

En realidad, los datos no siempre vienen con etiquetas, o es demasiado trabajo manual etiquetar o ni siquiera sabemos qu√© son las etiquetas. Piense en el esfuerzo que supondr√≠a etiquetar millones de im√°genes de carreteras para coches aut√≥nomos, es ah√≠ es cuando brilla el **Aprendizaje No Supervisado**.

En este sentido, el modelo no est√° supervisado y encuentra sus propios patrones.

---
### 1.6 Flujo de trabajo
---

Hasta ahora, sabemos que los datos de entrenamiento se usan para permitir que un modelo aprenda, luego ese modelo s epuede usar para hacer predicciones pero, **¬øcu√°les son los pasos intermedios?**

A continuaci√≥n se presentar√° el flujo de ejecuci√≥n del aprendizaje autom√°tico, que son cuatro pasos para construir un modelo.

Seguiremos los pasos con el siguiente escenario:
NYC publica sus registros mensuales de todos los apartamentos vendidos en la ciudad, incluye informaci√≥n sobre:

- Metros cuadrados del apartamento
- Nombre del vecindario
- A√±o de construcci√≥n
- Precio de venta
- etc.

Supongamos que queremos predecir el precio al que se vender√°n los apartamentos, por lo que  nuestro objetivo ser√° el precio de venta. Dado que tenemos estas etiquetas podemos darnos cuenta de que se trata de un problema de Aprendizaje Supervisado, veamos su flujo de trabajo.

### Flujo de trabajo:
1. **Extraer caracter√≠sticas**
   
   Los conjuntos de datos normalmente no vienen con caracter√≠sticas claras, por lo que hay trabajo por hacer para reformatear el conjunto de datos. Adem√°s, debe decidir con qu√© caracter√≠sticas desea comenzar. En nuestro caso, mencionamos algunos como: metros cuadrados, vecindario, a√±o, precio, etc. Sin embargo, hay m√°s que podr√≠an afectar a nuestro objetivo, como la distancia a la estaci√≥n de metro m√°s cercana.
  
2. **Dividir el conjunto de datos**
   
   Se divide en dos conjuntos de datos:
     1. Datos de prueba
     2. Datos de entrenamiento
  
        ![Paso 3 Entrenar el modelo](https://i.imgur.com/uAkTfGD.png)

3. **Entrenar el modelo**
   
   Para esto, el conjunto de datos del tren se ingresa en un modelo de aprendizaje autom√°tico elegido.
   
   ![Paso 3 Entrenar el modelo](https://i.imgur.com/iyparDW.png)

    Existen varios modelos diferentes de aprendizaje autom√°tico para elegir con diferentes casos de uso y niveles de complejidad, desde una red neuronal hasta una regresi√≥n log√≠stica.

4. **Evaluaci√≥n**
   
    No podemos asumir que el modelo ser√° funcional, entonces, ¬øcu√°l ser√≠a la mejor manera de evaluar el modelo? En este caso, nos gustar√≠a alimentar al modelo con las caracter√≠sticas conocidas de apartamentos vendidos y ver con qu√© precisi√≥n predice el precio de venta. No queremos utilizar ning√∫n dato usado para entrenar el modelo, porque le modelo ya ha visto esos datos, esto es exactamente para lo que es el conjunto de datos de prueba creado en el paso 2.

    ![Paso 4 evaluar el modelo](https://i.imgur.com/MpNKvl5.png)
    
    Alimentamos al modelo con el conjunto de datos de prueba, tambi√©n conocidos como "datos ocultos" (_unseen data_), y esperamos las predicciones del modelo.
    Hay muchas maneras de evaluar el rendimiento de nuestro modelo, por ejemplo, podr√≠amos calcular el error medio de las predicciones o el porcentaje de los precios de venta de apartamentos que se pronosticaron con precisi√≥n dentro de un margen del 10%.
    Cualquiera que sea el m√©todo que se elija, se debe decidir un umbral de rendimiento. Por ejemplo, supongamos que nuestro modelo predice con precisi√≥n el 80% de los apartamentos, aqu√≠ surge la pregunta obligatoria: **¬øEs mi modelo lo suficientemente bueno?**, en caso de obtener una respuesta afirmativa el modelo est√° listo para usarse, en el caso contrario habr√≠a que volver a entrenar el modelo y aplicar ciertas modificaciones (_tunes_), dicha smodificaciones puede significar un par de cosas diferentes, por ejemplo, ajustar las opciones o caracter√≠sticas del modelo.

    ![Paso 4 evaluacion de eficacia](https://i.imgur.com/f0OQgLC.png)

    Ajustar el modelo puede llevar un tiempo y si el rendimiento no mejora, muchas veces significa que no tiene suficientes datos.

    ![Paso 4 modelo final](https://i.imgur.com/IDUMOHV.png)

Esos son los 4 pasos del flujo de trabajo.

#### En resumen:
1. Extraer caracter√≠sticas
   - Elegir las caracter√≠sticas y manipular el conjunto de datos
  
2. Dividir el conjunto de datos
   - Dividirlo en dos conjuntos:
       1. datos para entrenamiento
       2. datos para pruebas

3. Entrenar el modelo
   - Entrenar el modelo con el conjunto de datos de entrenamiento creado en el paso 2 y un modelo de aprendizaje

4. Evaluar el modelo
   - Si el modelo no cumple con las expectativas, hay que aplicar "_tunearlo_" y repetir el paso 3 hasta obtener los resultados deseados.

---
## **Cap√≠tulo 2: M√©todos de Aprendizaje del _Machine Learning_**

2.1 Aprendizaje Supervisado
---

El aprendizaje supervisado es b√°sicamente una m√°quina de etiquetado, toma una observaci√≥n y le asigna una etiqueta.

Hay dos m√©todos de aprendizaje supervisado: 
1. **Clasificaci√≥n:** consiste en asignar una categor√≠a a una observaci√≥n, estamos prediciendo una variable discreta, una variable que s√≥lo puede tomar unos cuantos valores diferentes.
   - "¬øEste cliente detendr√° su suscripci√≥n?"
     - S√≠, No.
   - "¬øQu√© clase de vino es este?"
     - Rosado, Tinto, Blanco.
   - "¬øQu√© flor es esa?"
     - Rosa, Clavel, Tulip√°n.
   
   Por ejemplo, de la siguiente table tomemos las admisiones universitarias donde queremos predecir la aceptaci√≥n. Para simplificar, mostramos dos caracter√≠sticas: GPA y Resultados de la Prueba (_Test Results_).

   ![MLS clasificaci√≥n](https://i.imgur.com/0LrT84I.png)

   Se podr√≠a tener m√°s funciones, como: participaci√≥n en organizaciones estudiantiles, deportes o premios que hayan ganado los solicitantes, etc.

   El objetivo es lo que queremos prececir y hay dos etiquetas posibles _True_ o _False_, que quiere decir, si son aceptados en la universidad o no.

   ![MLS objetivo](https://i.imgur.com/5sETjqX.png)

   El objetivo s√≥lo puede contener una de esas dos etiquetas, lo que lo convierte en un problema de clasificaci√≥n.

   A continuaci√≥n se muestran las observaciones trazadas:

   ![MLS grafico estado de solicitudes](https://i.imgur.com/Ne5fLcM.png)
   _Gr√°fica sobre los aspirantes aceptados y no aceptados por la universidad._

   Los puntos azules representan a los solicitantes aceptados, los puntos rojos representan a los solicitantes rechazados.
   Mantenemos el 80% de nuestros datos para entrenar nuestro modelo.

   ![MLS grafico clasificaci√≥n manual de de solicitudes](https://i.imgur.com/mLO3SZ3.png)
   _Grafico clasificaci√≥n manual de solicitudes de aspirantes_

   Debido a que s√≥lo usamos dos funciones, podemos trazar e interpretar los resultados. Para nosotros, los humanos, es bastante claro; el gr√°fico demuestra que si se obtiene un puntaje superior a 4 tanto en el GPA como ne la prueba de ingreso, es aceptado. Si agregaramos m√°s funciones como extracurriculares o premios, necesitar√≠amos m√°s ejes y ser√≠a muy d√≠ficil para nosotros interpretar los datos con nuestro ojo humano. Sin embargo, un modelo no luchar√≠a en lo absoluto. Podr√≠amos usar una m√°quina de soporte de vectores, suena temible, pero s√≥lo es una l√≠nea que separa nuestros puntos. As√≠ entrenamos nuestro algoritmo y clasificamos el 20% de las observaciones que dejamos de lado.

   ![MLS grafico m√°quina de soporte de vectores - clasificaci√≥n l√≠neal de solicitudes de aspirantes](https://i.imgur.com/epxOcX8.png)
   _Grafico m√°quina de soporte de vectores: clasificaci√≥n l√≠neal de solicitudes de aspirantes_.
   
   S√≥lo clasifica err√≥namente dos puntos azules como rojos, lo que significa que se predijo err√≥neamente que dos solicitantes fueran rechazados.
   
   El problema es que intenta separarse con una l√≠nea recta, por lo que es poco probable que lo haga mejor que eso, pero, ¬øQu√© pasar√≠a si permitimos una l√≠nea curva? Hay maneras para modificar el comportamiento de un modelo, como permitir curvas, por ejemplo:

   ![MLS grafico m√°quina de soporte de vectores - clasificaci√≥n polinomial de solicitudes de aspirantes](https://i.imgur.com/nUhxL6u.png)
   _Grafico m√°quina de soporte de vectores: clasificaci√≥n polinomial de solicitudes de aspirantes_

   ¬°Ahora clasifica todo correctamente!


2. **Regresi√≥n:** Asigna una variable continua que puede tomar cualquier valor. Por ejemplo:
   - ¬øCu√°nto valdr√° esta acci√≥n?
   - ¬øQu√© tan alto ser√° ese ni√±o cuando crezca?
   - ¬øCu√°l ser√° la masa de √©ste exoplaneta?

   Para ejemplificar utilicemos lecturas meteorol√≥gicas para predecir la temperatura:

   ![MLC tabla lecturas meteorol√≥gicas](https://i.imgur.com/mzL5cpi.png)
   
   ¬øPodemos predecir la temperatura en funci√≥n del porcentaje de humedad?, usamos el 80% de los datos para entrenar nuestro modelo. Parece que cuando sube la humedad, baja la temperatura.

   ![MLC gr√°fica de evoluci√≥n de la temperatura basada en la humedad](https://i.imgur.com/U5IdryN.png)
   _Gr√°fica de evoluci√≥n de la temperatura basada en la humedad_

   De hecho, nuestro modelo de regresi√≥n lineal capta esto:

   ![MLC gr√°fica de evoluci√≥n de la temperatura basada en la humedad](https://i.imgur.com/GpPpj1Z.png)
   
   ![MLC gr√°fica de evoluci√≥n de la temperatura basada en la humedad 2](https://i.imgur.com/i0iySQG.png)
   _Gr√°ficas regresi√≥n l√≠neal sobre la evoluci√≥n de la temperatura basada en la humedad_

   Seg√∫n nuestro modelo, si el √≠ndice de humedad es 0.5, entonces la temperatura es de 18.5 C¬∞.

   Y es as√≠ como se comporta el modelo en datos reales. Identific√≥ la tendencia, pero sigue siendo malo para predecir. M√°s caracter√≠sticas como el viento, nubosidad, ubicaci√≥n, temporada, entre otras; podr√≠an hacerlo m√°s preciso.
  
   #### En resumen:
   - Regresi√≥n == Valores Continuos
     - Cualquier valor finito (altura) o intervalo infinito (tiempo) 
       - Una temperatura exacta: 20 C¬∞, 18 C¬∞, 45 C¬∞...
   
   - Clasificaci√≥n == Etiquetas de Categor√≠as
     - Uno de unos cuantos valores espec√≠ficos
       - Fr√≠o, templado, caluroso...
     - Beb√©, ni√±o, adolescente, adulto...

**El m√©todo de aprendizaje supervisado debe ser elegido a conciencia, considerando si desea enmarcar un problema como el problema como uno de regresi√≥n o uno de clasificaci√≥n.**

---
2.2 Aprendizaje No Supervisado
---

El Aprendizaje No Supervisado es bastante similar al Aprendizaje Supervisado, excepto que no tiene como objetivo una columna espec√≠fica, de ah√≠ la parte no supervisada.

### Entonces, ¬øc√∫al es el punto?
Lo que vuelve realmente interesante y poderoso al Aprendizaje No Supervisado es que aprende del conjunto de datos e intenta encontrar patrones, es decir, podemos encontrar informaci√≥n sin saber muhco sobre nuestro conjunto de datos, lo que lo hace ideal para tratar con cantidades enormes de datos d√≠ficiles de interpretar a _"ojo de buen cubero"_.



### 2.2.1 Aplicaciones
Puede tener diversas aplicaciones, principalmente la agrupaci√≥n, detecci√≥n de anomal√≠as y asociaciones. 

#### Agrupaci√≥n (_clustering_)
Consiste en identificar grupos en su conjunto de datos. Las observaciones en estos grupos comparten similitudes m√°s fuertes con miembros de su grupo, que con miembros de otros grupos.

Por ejemplo, d√≠gamos que tenemos un conjunto de datos con seis observaciones, ¬øQu√© grupos detectar√≠a el algoritmo?

![Aprendizaje No Supervisado: Agrupacion de especies](https://i.imgur.com/7e14qlW.png)
_Ejemplo del conjunto de datos_

Es una pregunta cuya respuesta depende de diversos factores.

Del conjunto de datos original pueden surgir dos grupos: "Perros" y "Gatos".

![Aprendizaje No Supervisado: Agrupacion de especies por raza](https://i.imgur.com/ipq5lvJ.png)

O bien, podr√≠a hacer cuatro grupos por color: negro, gris, blanco y caf√©.

![Aprendizaje No Supervisado: Agrupacion de especies por color](https://i.imgur.com/M1xHCWD.png)

Tambi√©n podr√≠a agrupar los elementos del conjunto de datos por pa√≠s origen, en este caso: "Europa" y "Jap√≥n".

![Aprendizaje No Supervisado: Agrupacion de especies por nacionalidad](https://i.imgur.com/mT5ZFZX.png)

Se ha ejemplificado lo que representa cada grupo, sin embargo, en la pr√°ctica no se suele tener conocimiento sobre qu√© diferencia a los _cl√∫steres_ (grupos); el modelo no le dir√° por qu√© o c√≥mo se decidi√≥ formar los grupos. Depender√° de usted investigar y averiguarlo.

**Modelos de _clustering_**
- _K Means_:
  - Requiere especificar de antemano el n√∫mero de cl√∫steres que le gustar√≠a identificar
- DSCAN _(density-based spatial clustering of applications with noise)_:
  - No requiere que especifique el n√∫mero de cl√∫steres por adelantado. En cambio, requieren que defina qu√© constituye un grupo, como el n√∫mero m√≠nimo de observaciones en un grupo.
  
D√≠gamos que tenemos flores de especies desconocidas y todo lo que conocemos es su ancho y largo del p√©talo:

![ANS Dataset](https://i.imgur.com/ZdlF7S4.png)

Ahora el problema de clasificaci√≥n se vuelve visible. Aqu√≠ no tenemos una columna con etiquetas de la especie, ni siquiera sabemos con cu√°les y cu√°ntas especies estamos hay.

Si planteamos la hip√≥tesis de que hay 4 especies, podemos usar _K Means_ y requerir 4 _clusters_ distintos.

![ANS K-Means 4 clusters](https://i.imgur.com/fbZCRki.png)

 Si suponemos que hay 3 especies, necesitamos 3 _clusters_ diferentes. Estos grupos son realmente correctos, ya que hay tres especies en el conjunto de datos: _Setosa, Virginica_ y _Versicolor_.

 ![ANS K-Means 3 clusters](https://i.imgur.com/CGy1itt.png)


 #### Detecci√≥n de anomal√≠as
La detecci√≥n de anomal√≠as consiste en detectar valores at√≠picos _(outliers)_.
Los valores at√≠picos son observaciones que difieren bastante del resto. En la siguiente imagen, todos los puntos est√°n agrupados en la parte inferior izquierda, excepto uno en la parte superior derecha.

![ANS Detecci√≥n de Anomal√≠as](https://i.imgur.com/EbXwLX0.png)

Resulta que ese pinto es la suma total de las otras observaciones , la fila total no se elimin√≥ antes de trazar los datos. En este caso, eliminar del conjunto de datos (_dataset_) el valor at√≠pico nos permitir√≠a apreciar de mejor manera el gr√°fico, tal como se muestra a continuaci√≥n.

![ANS Detecci√≥n de Anomal√≠as 2](https://i.imgur.com/RrmNrRM.png)

Con dos dimensiones, es sencillo encontrar valores at√≠picos a simple vista. Intente encontrar valores at√≠picos e 3 dimensiones; eso podr√≠a ser factible, pero, ¬øqu√© tal 4, 10, 20, 100?, es por eso que necesitamos m√©todos de aprendizaje no supervisados.

En el ejemplo anterior tratamos la anomal√≠a como un error, pero las anomal√≠as no deben ser tratadas siempre como errores, su detecci√≥n nos permitir√≠a, por ejemplo, descubrir qu√© dispositivos fallan m√°s r√°pido que otros, descubrir personas mal intencionadas que vulneran un sistema o alg√∫n paciente que sorprendentemente sea resistente a una enfermedad mortal. La detecci√≥n de anomal√≠as permite descubrir cualquier evento poco com√∫n en cualquier tipo de contexto.

#### Asociaci√≥n
La asociaci√≥n nos permite encontrar relaciones entre datos observados. En otras palabras, se trata de encontrar eventos que suceden juntos.

Un uso com√∫n de esta caracter√≠stica es el an√°lisis del un carrito de compras virtual, que se trata √∫nicamente de "¬øQu√© objetos se compran juntos?".

![ANS Asociaci√≥n: carrito de compras](https://i.imgur.com/H2xUTHt.png)

Por ejemplo, es probable que las personas que compran mermelada compren pan, las personas que compran  cerveza compre b√≥tanas y las personas que compren vino probablemente compren queso.


#### En resumen
- Aprendizaje No Supervisado = An√°lisis sin un objetivo espec√≠fico
  - Sin g√∫ia
  - An√°liza el conjunto de datos en su totalidad
  - Intenta detectar patrones
  - Agrupa las observaciones con funci√≥n en lo que se le solicite
  - Detecta anomal√≠as en las observaciones
  - Asocia las observaciones entre s√≠

---
2.3 Evaluaci√≥n del rendimiento
---

Recordemos el paso 4 del flujo de trabajo.

![Paso 4 modelo final](https://i.imgur.com/IDUMOHV.png)

Lo primero que se busca evaluar es el _overfitting_ (sobreajuste). Es **cuando nuestro modelo funciona muy bien con los datos de entrenamiento, pero mal con los datos de prueba.** 

Cuando esto ocurre significa que nuestro modelo aprendi√≥ el entrenamiento establecido de memoria y es incapaz de aplicar los aprendizajes a nuevos _datasets_, que es lo que originalmente quer√≠amos. Es por ello que necesitamos dividir nuestro conjunto de datos en dos conjuntos.


Por ejemplo, la l√≠nea verde aqu√≠ se sobreajusta, haciendo todo lo posible para clasificar los puntos perfectamente, por lo tanto, funciona muy bien en este conjunto de datos, pero deficientemente en datos no vistos.

![ER: overfitting ejemplo](https://i.imgur.com/WqD3R7H.png)

La l√≠nea negra comete m√°s errores en ese _dataset_, pero generaliza mejor.

#### _Accuracy_ (precisi√≥n)
En el ejemplo de la aceptaci√≥n universitaria, ¬øc√≥mo medir√≠amos el desempe√±o del modelo? Podr√≠amos usar la precisi√≥n (_accuracy_), que es el n√∫mero de las observaciones clasificadas correctamente (OCC) divididas entre la cantidad total de observaciones (CTO).

***_Accuracy_ = OCC / CTO***

![ER: Accuracy](https://i.imgur.com/6CmiXMM.png)

Sin embargo, la precisi√≥n no es siempre la mejor m√©trica para evaluar el rendimiento de un modelo.

Considere el fraude donde s√≥lo una peque√±a minor√≠a de transacciones son fraudulentas. D√≠gamos que entrenenamos un modelo para predecir si una transacci√≥n es fraudulenta o leg√≠tima.

Aqu√≠ hay un gr√°fico que muestra su rendimiento en 30 puntos de datos de prueba:

![ER: Limite de la precisi√≥n - ejemplo de fraude](https://i.imgur.com/SI8QJeI.png)

_Limite de la precisi√≥n: ejemplo de fraude_

S√≥lo clasifica err√≥neamente 2 puntos, lo que le da una precisi√≥n de alrededor del 93%, lo que suena bien. Pero el modelo en realidad pasa por alto la mayor√≠a de las transacciones fraudulentas, lo cual ser√≠a un problema si implementamos √©ste modelo en el mundo real. Incluso podemos decir que todos los puntos son leg√≠timos y obtendr√≠amos una precisi√≥n del 90%, pero no detectamos a todos los estafadores.

Para subsanar estos conflictos se introdujo la matr√≠z de confusi√≥n.

#### Matriz de Confusi√≥n

En el ejemplo, se predijo una transacci√≥n fraudulenta que en verdad era fraudulenta, de manera que se agrega un 1 a la matriz donde la columna Valores actuales 


|             ||          Valores Actuales          | Valores Actuales|
| --          | --              | --                | --              |
|             |                 | Fraudulentos      | No Fraudulentos |
| Predichos   | Fraudulentos    | 1                 |         0       |
| Predichos   | No Fraudulentos | 2                 |         27      |

![ER: Matriz de confusi√≥n](https://i.imgur.com/Pr2YeB9.png)

#### Interpretaci√≥n de la matriz
- Los **Verdaderos Positivos**, son puntos clasificados correctamente. En este caso, s√≥lo hay un punto fraudulento clasificado correctamente como fraudulento en el √°rea roja.

- Los **Falsos Negativos** so observaciones fraudilentas que se clasifican incorrectamente como leg√≠timas. En este caso se han clasificado 2, representadas por los 2 puntos rojos en el √°rea azul.
Los Falsos negativos son como decirle a una mujer embarazada que no est√° embarazada

- Los **Falsos positivos** son observaciones leg√≠timas que se clasifican incorrectamente como ileg√≠timas. En este caso hay 0.

- Los **Verdaderos Negativos** son observaciones leg√≠timas predichas clasificadas correctamente como leg√≠timas en la predicci√≥n. En este caso hay 27 puntos leg√≠timos predichos correctamente como no fraudulentos.

  Si sumamos todos los cuadrangulares de la matriz se deber√≠a obtener el total de observaciones, que en este caso es de 30 puntos.

#### Sensibilidad (_Sensitivity_)
Recordemos que est√°bamos en busca de una m√©trica que funcionara mejor que la presici√≥n (_accuracy_) para medir el rendimiento de nuestro modelo en nuestro caso espec√≠fico de fraude, y es aqu√≠ donde entra en juego la **sensibilidad**. Esta valora la predicci√≥n precisa de transacciones fraudulentas espec√≠ficamente valorando m√°s los verdaderos positivos.

**F√≥rmula de la Sensibilidad**

| Sensibildad |
| --          |
|Sensibilidad = Verdaderos Positivos / (Verdaderos Positivos + Falsos Positivos)|
|![ER: formula sensibilidad](https://i.imgur.com/BOhKuMk.png)|
|*_No es necesario aprenderla de memoria_*|
||

En este caso obtuvimos una sensiblidad del 33%, la cual es una mala puntuaci√≥n. **Optimizar la sensiblidad significa que preferimos marcar transacciones leg√≠timas como sosopechosas antes que autorizar transacciones fraudulentas.**

Ahora vemos que nuestro modelo no es bueno para predecir el fraude y necesita mejoras. 

#### Especifidad (_Specificity_)
Por otro lado tenemos la **especifidad** que valora los verdaderos negativos.

**F√≥rmula de la Especifidad (_Specificity_)**

| Especifidad |
| --          |
|Especifidad = Verdaderos Negativos / (Verdaderos Negativos + Falsos Positivos)|
|*_No es necesario aprenderla de memoria_*|
||

Esta es una m√©trica √∫til para los filtros de spam. Para un usuario es preferible enviar correo no deseado a la bandeja de entrada principal, antes que enviar correos reales a la carpeta de correo no deseado.

Esa es la clasificaci√≥n.

---
### Evaluaci√≥n de la Regresi√≥n
Esencialmente, queremos la diferencia entre el valor real y el valor predicho. Esta puede ser la distancia entre los puntos y la l√≠nea predicha.

![ER: Evaluacion de la regresi√≥n](https://i.imgur.com/P61Afdp.png)

Hay varias formas de calcular este error, como el error cuadr√°tico medio, pero esta es la idea general.

---

#### ¬øY qu√© sucede con el Aprendizaje No Supervisado?
Recuerde que √©ste no tiene variables predichas, por lo que no hay una salida correcta con la que comparar. El rendimiento de su modelo de aprendizaje no supervisado depende del problema que est√© resolviendo. Por lo tanto, usted eval√∫a el desempe√±o en funci√≥n de qu√© tan bien los resultados avanzan en su objetivo inicial.

---
2.4 Mejorando el desempe√±o
---

Ahora que ya sabes c√≥mo elegir entre clasificaci√≥n, regresi√≥n o t√©cnicas de aprendizaje no supervisado y c√≥mo evaluar sus resultados.

Despu√©s de evaluar nuestro modelo, tenemos que decidir si el rendimiento es lo suficientemente bueno.

![2.4 MD diagrama](https://i.imgur.com/uZGfJxn.png)

Existen varias opciones para mejorar su modelo en caso de que no est√© satisfecho con su rendimiento, pero nos centraremos en tres.

---
####  **2.4.1 Reducci√≥n de la dimensionalidad**

**Dimensi√≥n:** Una dimensi√≥n denota la cantidad de caracter√≠sticas (_features_) en sus datos.

| T√©cnica | Funcionamiento |
| ------- | -------------- |
Reducci√≥n de la<br>dimensionalidad | Elimina caracter√≠sticas completamente irreleventes. Demuestra que m√°s informaci√≥n no es siempre mejores predicciones.

**Ejemplo**  
Para predecir cu√°nto tiempo nos tomar√° llegar a la oficina; la hora del d√≠a y el clima son factores interesantes, pero es poco probable que la cantidad de vasos que bebimos ayer sea muy √∫til.

![2.4.1 ejemplo](https://i.imgur.com/sLfijXB.png)

Algunas caracter√≠sticas pueden estar altamente correlacionadas y llevar informaci√≥n similar, podr√≠amos mantener una sola caracter√≠stica y ag√∫n tener la mayor parte de la informaci√≥n.
Por ejemplo, la altura y el tama√±o de un zpato est√°n altamente correlacionados. Es muy probable que las personas altas tengan una talla de zapatos grande.  
Tambi√©n podr√≠amos colapsar varias funciones en una sola funci√≥n subyacente. Si tenemos dos caracter√≠sticas, como la altura y el peso podr√≠amos calcular una caracter√≠stica de √≠ndice de masa corporal en su lugar.

---
#### **2.4.2 Ajuste de Hiperpar√°metros**

**Hiperpar√°metro:**  
Es un par√°metro, del modelo, que uno elige antes de comenzar su entrenamiento.

antes de comenzar el entrenamiento.
Un modelo de aprendizaje autom√°tico es como una consola de producci√≥n musical, dependiendo de si est√°s mezclando m√∫sica _pop_, _rap_ o _heavy metal_ se deben aplizarlos ajustes pertinentes a los instrumentos y la voz.  
Algunos ajustes sonar√°n mejor con ciertos g√©neros que con otros. Bueno, en nuestro caso, el g√©nero es el conjunto de datos y la configuraci√≥n de los instrumentos son los hiperpar√°metros. Dependiedo del _dataset_, diferentes valores de hiperpar√°metros¬¥dar√°n mejores o peores resultados.

|T√©cnica | Funcionamiento|
|--------|---------------|
|Ajuste de <br> hiperpar√°metros|Se trata de personalizar los ajustes espec√≠ficamente para los requerimientos del modelo de aprendizaje autom√°tico.|

**Ejemplo**  
Cuando entrenamos la SVM en la primera lecci√≥n del cap√≠tulo, cambiamos de una l√≠nea recta a una curva.

![MLS grafico m√°quina de soporte de vectores - clasificaci√≥n l√≠neal de solicitudes de aspirantes](https://i.imgur.com/epxOcX8.png)

![MLS grafico m√°quina de soporte de vectores - clasificaci√≥n polinomial de solicitudes de aspirantes](https://i.imgur.com/nUhxL6u.png)

Eso es porque ajustamos el hiperpar√°metro "nucleo" de "lineal" a "polinomio"

<p align = "center">

|_kernel: 'linear' --> 'poly'_|
|----|
</p>

Hay muchos m√°s hiperpar√°metros que se pueden ajustar en el modelo SVM:
* C
* degree
* gamma
* shrinking
* coef-
* tol
* ...

Jugar con diferentes valores para cada hiperpar√°metro afectar√° el rendimiento de nuestro modelo. No tienes que adivinar combinaciones al azar, hay combinaciones estructuradas maneras de encontrar los valores √≥ptimos, pero eso est√° fuera del alcance de este curso.

---

#### **2.4.3 M√©todos de conjunto**

|T√©cnica| Funcionamiento |
|----|----|
|M√©todos de Conjunto|La √∫ltima opci√≥n que queremos cubrir son los m√©todos de conjunto. Esto es una t√©cnica que combina varios modelos para producir un modelo √≥ptimo.|

<br>

Imagina que estamos usando tres modelos diferentes: A, B y C.  

![MD 2.4.3 metodos de conjunto](https://i.imgur.com/wM14YVj.png)

En un entorno de clasificaci√≥n, usar√≠amos la votaci√≥n. Si el modelo A y C dicen que el estudiante es aceptado y el modelo B dice que no, entonces se acepta la observaci√≥n, ya que fue la predicci√≥n m√°s com√∫n entre todos los modelos.  

![MD 2.4.3 metodos de conjunto 2](https://i.imgur.com/bisCq8E.png)

En una configuraci√≥n de regresi√≥n, usamos el promedio. Si el modelo A predice una temperatura de 5¬∞, el modelo B una temperatura de 8¬∞ y el Modelo C una temperatura de 4¬∞, a la observaci√≥n se le asigna el valor promedio 5.67¬∞

![MD 2.4.3 metodos de conjunto 3](https://i.imgur.com/20iQH06.png)  

---
## **Cap√≠tulo 3: _Deep Learning_**

El _Deep Learning_ (aprendizaje profundo), utiliza un algoritmo llamado **_Neural Networks_** (Redes Neuronales), que est√°n vagamente inspirados en las redes neuronales biol√≥gicas del cerebro humano. Las neuronas, tambi√©n llamadas: "nodos", son la unidad b√°sica de las redes neuronales.  
El _Deep Learning_ es un tipo especial de aprendizaje autom√°tico que puede resolver m√°s problemas complejos, pero requiere muchos m√°s datos que el aprendizaje autom√°tico tradicional. Se utiliza mejor en los casos en que las entradas est√°n menos estructuradas, como grandes cantidades de texto o im√°genes.  

---
### **3.1 Funcionamiento**

---
Imagina que trabajas para un estudio de _Hollywood_ y quieres predecirlos ingresos de taquilla de una pr√≥xima pel√≠cula.  
Tiene acceso a un conjunto de datos que mapea los ingresos de taquilla de pel√≠culas pasadas a su presupuesto de producci√≥n.  

Como puede ver, se puede dibujar una l√≠nea recta a trav√©s de los datos, lo que demuestra que a mayor presupuesto, mayor ingresos por taquilla. La l√≠nea roja es un ejemplo de una predicci√≥n de un modelo simple.

![DL:_3.1_Funcionamiento](https://i.imgur.com/43DZ45l.png)  

Una red neuronal act√∫a de manera distinta para realizar predicciones. En este caso, la red neuronal que lograr√≠a esto se puede dibujar de la siguiente manera:

![DL:3.1_Funcionamiento_2](https://i.imgur.com/Xph4fuo.png)

El presupuesto se pasa como entrada a una neurona que calcula la curva roja y genera ingresos de taquilla.

Suponga que ha obtenido acceso a m√°s informaci√≥n. Adem√°s del presupuesto de producci√≥n, tambi√©n save cu√°nto ha gastado el estudio en publicidad, cu√°l es el "poder estelar" (influencia de los actores) determinado por el n√∫mero de seguidores en Twitter de los actores, por ejemplo, y el momento del estreno de la pel√≠cula. Veamos c√≥mo luce una red neuronal m√°s compleja.

Primero, considere esta neurona, cuyo trabajo es estimar el gasto en funci√≥n del presupuestyo y los costos publicitarios.

![DL:3.1_Funcionamiento_RN_3](https://i.imgur.com/T7wRF89.png)

La segunda neurona reastrea qu√© tan enteradas est√°n las personas de que la pel√≠cula se ha estrenado. Las dos cosas que alimentan eso son la publicidad y el poder estelar. Cuanto m√°s famosos sean los integrantes del reparto, m√°s gente estar√° al tanto de la pel√≠cula.

![DL:3.1_Funcionamiento_RN_4](https://i.imgur.com/iNPhF1K.png)

As√≠ que la segunda neurona es responsable de la conciencia.

![DL:3.1_Funcionamiento_RN_5](https://i.imgur.com/f88kJmu.png)

Por √∫ltimo, entrar√°n en juego las decisiones de distribuci√≥n que tome el estudio. El presupuesto, la publicidad y el momento de lanzamiento se alimentan de esta neurona, que representa la distribuci√≥n de la pel√≠cula.

![DL:3.1_Funcionamiento_RN_6](https://i.imgur.com/LwhUMKl.png)

![DL:3.1_Funcionamiento_RN_7](https://i.imgur.com/71JmyV9.png)

Finalmente, ahora que las neuronas anteriores han descubierto la importancia de estos conceptos de nivel superior, necesitamos agregar una neurona m√°s que tome estos tres factores como entrada y genere los ingresos de taquilla estimados.

![DL:3.1_Funcionamiento_RN_8](https://i.imgur.com/Z5MBNWK.png)

As√≠ es como nuestra red neuronal est√° formada y lista para predecir el ingreso taquillero.  

Su trabajo es mapear las relaciones entre diferentes combinaciones de variables al resultado deseado.

A partir de la explicaci√≥n parec√≠a que tuvi√©ramos que averiguar relaciones clave como el gasto, notoriedad y la distribuci√≥n.

![DL:3.1_Funcionamiento_RN_9](https://i.imgur.com/qMN1MRp.png)

Para entrenar una red neuronal, todo lo que necesita es tener un conjunto de datos de entrenamiento. El algoritmo analiza y descubre por s√≠ mismo todas las conexiones entre las neuronas.

El ejemplo anterior se trata de una red neuronal bastante simple, en realidad las redes neuronales son mucho m√°s largas e incluyen miles de neuronas; llegados a ese punto, se usa el t√©rmino _Deep Learning_.  

![DL:3.1_Funcionamiento_RN_9](https://i.imgur.com/FIBoxzj.png)
_Representaci√≥n de una Red Neuronal_

---
#### **3.2 Cu√°ndo Utilizar _Deep Learning_**

---

Se recomienda utilizar el algoritmo de _Deep Learning_ en los siguientes casos:

1. El tama√±o de los datos que se desea analizar es demasiado grande  

2. Se cuenta con una PC potente 
   
3. Cuando no se tiene conocimiento de los datos que se van a procesar (la red neuronal resuelve los problemas por ti)

4. Cuando se trata de problemas complejos como:
   1. Visi√≥n artificial
   2. Procesamiento del lenguaje natural

---
#### Resumen
_Deep Learning_  
- AKA: _Neural Networks_
  - Unidad B√°sica: Neurona
  - Las neuronas se alimentan de otras neuronas
  - Son bastante grandes
  - Poseen muchas neuronas
  - Resuelven problemas complejos
- √Årea especial del _Machine Learning_
- Requiere m√°s datos
- Es buena opci√≥n si los _inputs_ son im√°genes o texto
- Si el los datos a analizar no son muchos, se recomienda utilizar algoritmos tradicionales de aprendizaje autom√°tico
- Requiere computadoras poderosas para entrenar en un tiempo razonable

---
### **3.3 Procesamiento**

---

En el cap√≠tulo anterior se habl√≥ de que el _Deep Learning_ es especialmente √∫til cuando se trata de procesar im√°genes y texto. En √©ste cap√≠tulo nos centraremos en las im√°genes y c√≥mo se utilizan en las aplicaciones de visi√≥n artificial.

---
#### 3.3.1 Visi√≥n Artificial _(Computer Vision)_
---

El objetivo de la visi√≥n artificial es ayudar a entender a las computadores el contenido de las im√°genes digitales. La visi√≥n artificial es necesaria para habilitar, por ejemplo, los coches aut√≥nomos üöóüöô  
Fabricantes como Tesla, BMW, Volvo y Audi utilizan m√∫ltiples c√°maras para adquirir im√°genes del entorno para que los autom√≥viles puedan detercar objetos y se√±alamientos de tr√°nsito para conducir con seguridad. 

![DL:3.3.1_VA](https://i.imgur.com/A0lcDtq.png)

---
Para entender c√≥mo funciona, primero debemos saber c√≥mo se ven los datos de una imagen.

Una imagen est√° compuesta por p√≠xeles, cada p√≠xel contiene informaci√≥n sobre el color y la intensidad. En la imagen puedes apreciar una imagen en escala de grises pixelada. Cada pixel puede representar la intensidad con un valor entero que va desde el 0 hasta el 255.

![DL:3.3.1_VA_2](https://i.imgur.com/4FWtn8x.png)

---

En cuanto a las im√°genes a color, est√°n representadas por medio del Sistema RGB, RGB son las iniciales de los colores _rojo_, _verde_ y _azul_ en ingl√©s.  
Cada imagen puede ser representada por 3 cuadr√≠culas, una de cada color del sistema RGB. Esto sifnifica que se necesita tres veces la cantidad de datos para almacenar una imagen a color que una imagen en escala de grises (como la anterior).

![DL:3.3.1_VA_3](https://i.imgur.com/EDXdH7F.png)

De manera que las imagenes digitales pueden ser vistas como un mont√≥n de n√∫meros y esos n√∫meros pueden ser utilizados como caracter√≠sticas para un modelo de _Machine Learning_.

---

Imagina que quieres hacer un sistema para reconocer a las personas de las fotograf√≠as.

El paso n√∫mero uno es conseguir algunas fotograf√≠as y usarlas como entradas para dicho modelo.

![DL:3.3.1_VA_4](https://i.imgur.com/Kweo6xq.png)

La el valor que representa la intensidad de cada pixel se pasar√° a una red neuronal y su trabajo ser√° identificar a la persona de la fotograf√≠a. Entonces las neuronas intermedias entre las neuronas de entrada y las de salida procesar√°n los valores por s√≠ mismas; t√≠picamente cuando se alimenta una red neuronal con imagenes, las neuronas de las primeras fases del algoritmo aprenden a detectar bordes, despu√©s los bordes de las objetos, como los ojos y nar√≠z, por ejemplo, mientras que las neuronas de las etapas complementarias aprender√°n a detectar la forma de los rostros. Al final, la red neuronal reunir√° toda esta informaci√≥n para identificar a la persona de la imagen üíÅüèø‚Äç‚ôÄÔ∏è

No olvides que parte de la magia de las redes neuronales es que no necesitas preocuparte por entender lo que sucede _'under the hood'_ en la secci√≥n intermedia de las de la red neuronal. Lo √∫nico que necesitas hacer es darle muchas im√°genes de caras, las caracter√≠sticas, as√≠ como la identidad correcta, las etiquetas y durante el entrenamiento el algoritmo descubrir√° por s√≠ mismo cu√°l de las neuronas del medio deber√≠a estar trabajando üñ•Ô∏èüß†üí•

![3.3.1_VA_5](https://i.imgur.com/9gcYfhG.png)

---

Muchas aplicaciones y empresas bastante populares utilizan esta tecnolog√≠a de la visi√≥n artificial. Por ejemplo: 
- Facebook cuando detecta tu rostro en una fotograf√≠a publicada

- Tesla en sus coches aut√≥nomos para evitar siniestros

- Esc√°neres de tomograf√≠a computarizada para identificar tumores durante ex√°menes m√©dicos

- entre muchas otras tecnolog√≠as con las que convivimos d√≠a a d√≠a

---

Aunque la aplicaci√≥n de la tecnolog√≠a de la visi√≥n artificial va m√°s all√° que s√≥lo comprender e identificar im√°genes, tambi√©n es capaz de generar im√°genes s√∫per realistas y de toda clase de estilos art√≠sticos. Por ejemplo, __Deep Fake__, es un _software_ que se usa para poner el rostro de las personas en videos en los que ni siquiera aparecen. _Deep Fake_ entiende cu√°les son las caracter√≠sticas del rostro humano y gracias a ello puede generar caras nuevas y/o de alguien m√°s y agregar movimiento.

_Deep Fake_ gifs: 

![VA:3.2.1_DF1](https://media.giphy.com/media/Tb3pSooj6OvUVDgZli/giphy.gif)
![VA:3.2.1_DF2](https://media.giphy.com/media/ee8P9T5yW9eBJMhHSM/giphy.gif)
![VA:3.2.1_DF3](https://media.giphy.com/media/8cKrUOJD5RdSnCHbkp/giphy.gif)
![VA:3.2.1_DF3](https://i.imgur.com/6O8ktW2.png)

---
#### __Nota ajena al curso__
Llegados a √©ste punto quisiera invitar al lector a hacer una labor de instrospecci√≥n y pensar en las implicaciones √©ticas que conlleva utilizar estos tipos de programas üëΩ

![VA:3.2.1_jesus_is_watching_you](https://i.imgur.com/NHj1xJj.png)

---

## 3.4 Procesamiento del Lenguaje Natural _(Natural Language Processing)_

Anteriormente se mencion√≥ que el _Deep Learning_ funciona especialmente bien cuando se trata de procesar im√°genes y texto. Ya aprendimos sore el procesamiento de im√°genes y la Visi√≥n Artificial, ahora es el turno del procesamiento del texto como un lenguaje natural

El __Procesamiento del Lenguaje Natural__ o __NLP__ (por sus siglas en ingl√©s) es la capacidad de las computadoras para entender el significado de las sentencias, oraciones y palabras que conforman el lenguaje humano.

En la siguiente imagen se puede apreciar c√≥mo por medio del NLP la computadora es capaz de localizar y clasificar las palabras por su propia naturaleza en sus respectivas categor√≠as predefinidas, como los nombres de personas y ubicaciones.

![3:NLP_1](https://i.imgur.com/4BPcE8I.png)

### 3.4.1 T√©cnica NLP: __*Bag of Words*__

Al alimentar un modelo entradas de tipo texto, la manera m√°s simple de procesarle es contar cu√°ntas veces aparecen palabras importantes en una pieza de texto. √âsta t√©cnica es llamada **Bag of Words** üëú 

![3.1:Funcionamiento_Bag_of_Words](https://i.imgur.com/NhFvzPF.png)


Supongamos que queremos analizar las siguientes sentencias:

1. "U2 es una gran banda! ü§ò"
2. "Queen es una gran banda! ü§ò"


|Palabras|Sentencia #1|Sentencia #2|
|---|---|---|
||_"U2 es una gran banda"_|_"Queen es una gran banda"_|
|U2|1|0|
|Queen|0|1|
|es|1|1|
|una|1|1|
|gran|1|1|
|banda|1|1|
---
#### *__Bag of words: n-grams__*

Ahora veamos la siguiente sentencia:
1. "Ese libro no es bueno"

|Palabras|No. de apariciones|
|---|---|
|Ese|1|
|libro|1|
|no|1|
|es|1|
|bueno|1|
---

Al contar las palabras de manera individual, "bueno" es agregado a la lista, a pesar de que el sentimiento que se transmite en la oraci√≥n es justamente lo opuesto a ser bueno, esto puede ser resuelto contando las secuencias de las palabras; a √©sta t√©cnica se le conoce como **_'n-grams'_**. 
<br>

---

#### **_2-gra (bi-gram)_**


|Palabras|No. de apariciones|
|---|---|
|Ese libro|1|
|libro no|1|
|no es|1|
|es bueno|1|
---

Aqu√≠ estamos cotando palabras en secuencias de pares, lo que nos permite obtener m√°s informaci√≥n.  

_Bag of Words_ es una t√©cnica bastante √∫til y utilizada para el NLP. Tiene lagunas limitaciones, pero su simpleza permite obtener resultados bastante impresionantes.

---
### 3.4.1.2 Limitaciones y Propiedades

- El conteo de palabras no considera sin√≥nimos
  - Por ejemplo:
    - azul
    - azul-Cielo
    - aqua
    - lapizlazuli
    - cer√∫leo

Para solucionar ese conflicto se utiliza una t√©cnica llamada **_Word Embeddings_**. Lo que hace es crear relaciones entre palabras similares con el fin de crear una categor√≠a espec√≠fica para dichos grupos y otorgarles la misma caracter√≠stica. As√≠ todas las variantes de azul que se mencionaron en el ejemplo, se agruparan como "colores azulados" y con esto superar la limitante de los sin√≥nimos.  

Otra propiedad interesante de los **_Word Embeddings_** es que son representaciones matem√°ticas de palabras que siguen reglas intuitivas.

Por ejemplo, si tomamos las caracter√≠sticas de "Rey", substraemos las caracter√≠sticas de "Hombre" y agregamos las caracter√≠sticas de "Mujer", as√≠ nos acercamos bastante a un _set_ de caracter√≠sticas cercanas a las de "Reina".

![NLP:3.1_word_embeddings](https://i.imgur.com/j2BFC10.png)

*<p align="center"> Rey - Hombre + Mujer = **Reina**</p>*

---

### 3.5 Traducci√≥n del lenguaje

Despu√©s de mapear palabras u oraciones a n√∫meros, con la t√©cnica __*Bag of Words*__ podemos darlos como entrada a una red neuronal cuyo trabajo es traducir la oraci√≥n de entrada a un idioma distinto. Aqu√≠ tenemos la frase holandesa _"Met of zonder jou"_, traducida como: _"With or without you"_ (con o sin ti).

![NLP:3.5_1](https://i.imgur.com/QbbDJAu.png)

El procesamiento del lenguaje natural es la fuerza impulsora detr√°s de las siguientes aplicaciones comunes: 
- Traductores
  - _Google Translate_
- _Chatbots_ entre clientes y empresa
- Asistentes personales
  - Siri
  - Alexa
- An√°lisis de sentimientos, usado para cuantificar cu√°n positiva o negativa es la emoci√≥n expresada por un segmento de texto
- Entre muchas otras
---

### Resumen

<h4>¬øPor qu√© raz√≥n se prefiere el <i>Deep Learning</i> cuando se trabaja con datos de imagen y texto?</h4>  

1. Son problemas son bastante complejos y las redes neuronales son mucho mas eficientes que los algoritmos tradicionales de aprendizaje autom√°tico.

2. A menudo no est√° claro cu√°les deber√≠an ser las caracter√≠sticas del modelo para los datos de texto e imagen. El _Deep Learning_ no requiere intervenci√≥n humana y puede aprender las caracter√≠sticas por s√≠ solo, como qu√© p√≠xeles forman una nariz 

3. Cuando se trabaja con este tipo de datos, suele haber cantidades inmensas de ellos. Incluso una sola imagen puede constar de millones de p√≠xeles y un cuerpo de texto puede contener millones de palabras. Los algoritmos tradicionales de aprendizaje autom√°tico se quedan atr√°s al intentar procesar cantidades m√°sivas que s√≥lo continuan incrementando, mientras que el rendimiento del modelo un modelo de _Deep Learning_ incrementa a la par que incrementa la cantidad de datos.  
<br>   

---

#### Conceptos

- __*Natural Language Processing (NLP)*__: es la capacidad de las computadoras para entender el significado del lenguaje humano.
  
- **_Word Embeddings_**: capturan el significado contextual de las palabras e identifica si existe una relaci√≥n entre las palabras para agruparlas por categor√≠as; permitiendo superar as√≠ el impedimento de procesar sin√≥nimos de la t√©cnica **_Bag of Words_**. 

---
## 4. L√≠mitantes del _Machine Learning_

Hasta ahora hemos visto la amplia gama de posibilidades que nos ofrecen los algoritmos del _Machine Learning_, pero tambi√©n tiene sus limitaciones.

---

### 4.1 Calidad de los datos

Una frase conocida en el √°mbito del _Machine Learning_ es:
<p align="center"> <b><i>"Entra basura, sale basura"</i></b></p>

![4.1:Calidad_datos](https://i.imgur.com/cjSTKNf.png)

B√°sicamente significa que la calidad de la informaci√≥n de salida que arroje el modelo, ser√° tan buena como la calidad de los datos de entrada. Con datos de calidad precaria se obtendr√°n resultados desacertados, incompletos o incoherentes. 

---

### 4.1.1 Casos de fracaso

A continuaci√≥n veremos algunos casos que servir√°n para ejemplificar y comprender las consecuencias que puede ocasionar un mal entrenamiento para un modelo de _Machine Learning_.

### Caso Amazon
Entre el 2014 y 2017, el departamento de recursos humanos de Amazon utiliz√≥ un programa que hac√≠a uso de Inteligencia Artifical (IA) para apoyar el proceso de reclutamiento de personal; el programa revisaba los curr√≠culums (CVs) recibidos y hac√≠a recomendaciones espec√≠ficas.

Despu√©s se descubri√≥ que el modelo prefer√≠a a los candidatos masculinos sobre los femeninos debido a que estaba entrenado con los CVs que hab√≠a recibido la compa√±√≠a durante la √∫ltima d√©cada, y durante esa d√©cada se hab√≠an contratado muchos m√°s hombres que mujeres.

El modelo degradaba los curr√≠culos que conten√≠an la palabra "mujer", por ejemplo, al detectar que el candidato era una mujer o que hab√≠a asistido a una universidad para mujeres.

### Caso Microsoft: _AI Chatbot_

Microsoft fue noticia en 2016 cuando anunci√≥ su nuevo _chatbot_ "Tay", el cual podr√≠a responder autom√°ticamente a las personas y entablar una conversaci√≥n informal en _Twitter_.

A medida que m√°s personas conversaran con Tay, el _chatbot_ aprender√≠a a mantener mejores conversaciones. Lo que ocurri√≥ en menos de 24 horas despu√©s del lanzamiento de Tay, los _trolls_ de internet hab√≠an corrompido la personalidad del _chatbot_. Tay comenz√≥ a twittear cosas bastante ofensivas. Su capacidad innata para aprender signific√≥ que internalitz√≥ parte del lenguaje que aprendi√≥ de los _trolls_.

El bot termin√≥ publicando _twits_ sumamente racistas y ofensivos para la poblaci√≥n afroam√©ricana y la jud√≠a. Adem√°s de expresar su apoyo hacia el l√≠der que lider√≥ el gobierno alem√°n durante la Segunda Guerra Mundial; hasta que terminaron desactiv√°ndolo.

![4.1:microsoft_chatbot](https://i.imgur.com/fWZm3Xp.png)
![4.1:microsoft_chatbot](https://i.imgur.com/1WHBLnS.png)

---

Si bien es cierto que estos sucesos podr√≠an clasificarse como rotundos fracasos, son eventos que nos han servido para aprender y multiplicar el cuidado a la hora de alimentar con datos a los modelos de _Machine Learning_ y no confiar ciegamente en ellos.

---

### 4.1.2 C√≥mo asegurar la calidad de los datos

Datos de alta calidad requiere:

- An√°lisis de datos
  - Inclu√≠das:
    - Caracter√≠sticas
    - Distribuci√≥n
    - Fuente
    - Relevancia

- Revisi√≥n de valores at√≠picos
  - Excepciones
  - Cualquier cosa que se destaque como sospechosa

- Dominio de conocimientos
  - Pericia para explicar patrones de datos inesperados

- Documentaci√≥n
  - Proceso:
    - Transparente
    - Repetible

---

### 4.2 Explicabilidad
---

La segunda limitaci√≥n que discutiremos es la explicabilidad.

![4.2:explicabilidad](https://i.imgur.com/Jef2HH0.png)

Uno de los mayores desaf√≠os de la IA es que, a menudo, los modelos de aprendizaje autom√°tico se consideran cajas negras. Sin embargo, a veces es necesario que lis sistemas sean transparentes sobre el razonamiento que utiliza, para aumentar la confianza, la claridad y la comprensi√≥n.


![4.2:explicabilidad_2](https://i.imgur.com/YWTKKyy.png)

Por ejemplo, tendr√°s que ser capaz de explicar tu modelo para obtener la aceptaci√≥n empresarial de un cliente, demostrar que est√°s cumpliendo con las leyes en relaci√≥n a los datos, y permitir una detecci√≥n de sesgos m√°s r√°pida y efectiva. 

----

### 4.2.1 **_Explainable AI_** (IA Explicativa)

---

Durante √©ste cap√≠tulo hemos estado hablando sobre el _Deep Learning_, pero no se ha discutido sobre el gran inconveniente que es la falta de explicabilidad. Aunque el _Deep Learning_ puede hacer predicciones muy precisas, no siempre est√° claro por qu√© el modelo hace una predicci√≥n espec√≠fica.

Los m√©todos que nos permiten comprender los factores que conllevan al modelo a cada predicci√≥n tambi√©n se como conoce como **_Explainable IA_**.

Examinemos un problema t√≠pico en la IA explicable.  
Supongamos que un hospital est√° utilizando un modelo tradicional de aprendizaje autom√°tico para analizar los datos de los pacientes con diabetes.


---

**Esta es la parte explicable**  
El modelo entrenado puede decirnos dos cosas:

1. Puede predecir la aparici√≥n de diabetes tipo 2
   - **Predicci√≥n:** El paciente tendr√° diabetes?

2. Puede decir qu√© caracter√≠sticas fueron importantes para tomar tal decisi√≥n
   - **Inferencia:** Por qu√© ocurrir√° esto  
---
<br>
Esta explicabilidad adicional puede proporcionar informaci√≥n importante para los m√©dicos, como si la presi√≥n arterial fuera un predictor importante de diabetes en el futuro 

---

Compare ese ejemplo con un problema t√≠pico de _Deep Learning_.

![4.2.1_explicabilidad_3](https://i.imgur.com/i8JOQ0a.png)

Supongamos que queremos reconocer las letras escritas a mano.

---
**Predicci√≥n:** ¬øQu√© letra es?

---
Realmente no nos importa por qu√© una imagen en particular se clasific√≥ como "A", siempre que las predicciones sean muy precisas.

El _Deep Learning_ es una soluci√≥n perfecta para este problema porque no nos importa la explicabilidad en este caso. 

---
#### Resumen

- Basura entra <--> Basura sale
  
- La calidad de las salidas depender√° de la calidad de las entradas

- Tu modelo ser√° tan bueno como los datos con el que lo alimentes
  
- _Explainable IA_: Se refiere a la capacidad de comprensi√≥n de las predicciones de los modelos

- Utilizar _Deep Learning_ si no se requiere explicar el procedimiento por el cual se lleg√≥ a las predicciones del modelo

---
## Repaso
---

- Cap√≠tulos
  1. _¬øQu√© es el Machine Learning?_
     - Definici√≥n y relaci√≥n con la ciencia de datos y la inteligencia artificial
     - Fundamentos y flujo de trabajo
     ![Paso 4 modelo final](https://i.imgur.com/IDUMOHV.png)

  2. M√©todos de Aprendizaje del _Machine Learning_
     - Tipos de _Machine Learning_
     - Evaluaci√≥n y mejora de los modelos de _Machine Learning_
     - ![Repaso:cap_2](https://i.imgur.com/qQbERiw.png)

  3. _Deep Learning_
     - Visi√≥n Artificial
     - Procesamiento de Lenguaje Natural
     - Casos de uso
     ![Repaso:cap_3](https://i.imgur.com/LAE2Pns.png)

  4. Limitantes del _Machine Learning_
     - Calidad de los datos
     - Casos de Fracaso
     - Explicabilidad

---
### Felicitaciones!
Hasta aqu√≠ ha llegado el contenido de √©ste curso.

Si aprendiste algo nuevo si√©ntete orgulloso de ser menos ignorante que ayer que cuando empezaste a leer.

---

